import os
import re
import json
import hashlib
import asyncio
import logging
from typing import List, Optional, Dict
from dataclasses import dataclass

# âœ… æ›¿æ¢ä¸º OpenAI SDK (DeepSeek å…¼å®¹)
from openai import AsyncOpenAI
from tqdm.asyncio import tqdm_asyncio

# ================= é…ç½®åŒºåŸŸ =================
API_KEY = "sk-114514"  # æ›¿æ¢ä½ çš„ DeepSeek API Key
BASE_URL = "https://api.deepseek.com" # DeepSeek å®˜æ–¹åœ°å€

# æ¨¡å‹é€‰æ‹©ï¼š
# deepseek-chat (å³ DeepSeek-V3, æ¨èï¼Œé€Ÿåº¦å¿«ï¼Œæ–‡é‡‡å¥½)
# deepseek-reasoner (å³ DeepSeek-R1, é€»è¾‘å¼ºï¼Œä½†ä½œä¸ºæ–‡å­¦é‡å†™å¯èƒ½ç•¥æ…¢ä¸”è´µ)
MODEL_NAME = "deepseek-chat"

# ã€æ–‡ä»¶è·¯å¾„é…ç½®ã€‘
FILE_28_RAW = "hlm_28.txt"       # å¾…æ¶¦è‰²çš„å28å›åŸå§‹æ–‡ä»¶
TEMP_DIR = "temp_chapters_split" # ä¸´æ—¶å­˜æ”¾æ‹†åˆ†åç« èŠ‚çš„ç›®å½•
OUTPUT_DIR = "refined_chapters_pro_ds" # è¾“å‡ºç›®å½•æ”¹ä¸ªåï¼ŒåŒºåˆ†ä¸€ä¸‹
CACHE_FILE = "refiner_cache_ds.json"   # ç¼“å­˜æ–‡ä»¶ä¹ŸåŒºåˆ†ä¸€ä¸‹

CHUNK_SIZE = 2000
CONCURRENCY_LIMIT = 3  # DeepSeek API é™é€Ÿè¾ƒä¸ºä¸¥æ ¼ï¼Œå¦‚é‡ 429 è¯·è°ƒä½æ­¤æ•°å€¼ (å¦‚ 2 æˆ– 3)

# ================= é»„é‡‘é£æ ¼é”šç‚¹ (Golden Anchors) =================
GOLDEN_SAMPLES = """
ã€èŒƒä¾‹ä¸€Â·äººç‰©ç¥æ€ï¼ˆè„‚ç²‰æ°”ï¼‰ã€‘
å®ç‰å¬äº†ï¼Œæ­¤æ—¶å¿ƒä¸‹æ—©å·²æ˜ç™½äº†ï¼Œä¹Ÿä¸ç­”è¨€ï¼Œåªå˜»å˜»çš„ç¬‘ã€‚é»›ç‰é“ï¼šâ€œä½ ä¹Ÿä¸ç”¨å“„æˆ‘ã€‚ä»ä»Šä»¥åï¼Œæˆ‘ä¹Ÿä¸æ•¢äº²è¿‘äºŒçˆ·ï¼ŒäºŒçˆ·ä¹Ÿå…¨å½“æ²¡æœ‰æˆ‘è¿™ä¹ˆä¸ªäººï¼Œä¾¿æ˜¯ã€‚â€è¯´ç€ï¼Œé‚£æ³ªç å„¿æ—©æ‰‘ç°Œç°Œæ»šä¸‹æ¥äº†ã€‚

ã€èŒƒä¾‹äºŒÂ·åœºæ™¯æå†™ï¼ˆå¯Œè´µé”¦ç»£ï¼‰ã€‘
åªè§å…¥é—¨ä¾¿æ˜¯æ›²æŠ˜æ¸¸å»Šï¼Œé˜¶ä¸‹çŸ³å­æ¼«æˆç”¬è·¯ã€‚ä¸Šé¢å°å°ä¸¤ä¸‰é—´æˆ¿èˆï¼Œä¸€æ˜ä¸¤æš—ï¼Œé‡Œé¢éƒ½æ˜¯åˆç€åœ°æ­¥æ‰“å°±çš„åºŠå‡ æ¤…æ¡ˆã€‚ä»é‡Œé—´æˆ¿å†…åˆå¾—ä¸€å°é—¨ï¼Œå‡ºå»åˆ™æ˜¯åé™¢ï¼Œæœ‰å¤§æ ªæ¢¨èŠ±å…¼ç€èŠ­è•‰ã€‚

ã€èŒƒä¾‹ä¸‰Â·å™äº‹è½¬æŠ˜ï¼ˆè‹å‡‰æ„Ÿï¼‰ã€‘
ä¸”è¯´é‚£è´¾é›¨æ‘æ­¤æ—¶å®˜å¤åŸèŒï¼Œæ­£æ˜¥é£å¾—æ„ï¼Œå“ªé‡Œè®°å¾—å½“å¹´çš„è‘«èŠ¦æ¡ˆï¼Ÿæ­£æ˜¯ï¼šèº«åæœ‰ä½™å¿˜ç¼©æ‰‹ï¼Œçœ¼å‰æ— è·¯æƒ³å›å¤´ã€‚è¿™æ—¥æ­£åå ‚ï¼Œå¿½å¬æœ‰äººå‡»é¼“ï¼Œä½ é“æ˜¯è°ï¼Ÿ
"""
# ================= æ—¥å¿—é…ç½® =================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%H:%M:%S"
)
logger = logging.getLogger("RedRefiner-DS")

@dataclass
class SegmentTask:
    chapter_title: str
    index: int
    total: int
    text: str

class RedChamberEngine:
    def __init__(self, api_key: str, base_url: str, model_name: str):
        # âœ… åˆå§‹åŒ– OpenAI å¼‚æ­¥å®¢æˆ·ç«¯
        self.client = AsyncOpenAI(api_key=api_key, base_url=base_url)
        self.model_name = model_name
        
        self.system_prompt = (
            "ä½ æ˜¯ä¸€ä½çº¢å­¦é€ è¯£ææ·±çš„æ–‡å­¦å®¶ï¼Œæ­£åœ¨å°†ã€Šçº¢æ¥¼æ¢¦ã€‹ç™¸é…‰æœ¬çš„ç²—ç³™åº•ç¨¿ï¼Œé‡å†™ä¸ºæ›¹é›ªèŠ¹å‰å…«åå›çš„ç²¾ç¾æ­£æ–‡ã€‚\n"
            "ã€ç»å¯¹ç¦ä»¤ã€‘ï¼š\n"
            "1. ä¸¥ç¦ä¿®æ”¹åŸå‰§æƒ…èµ°å‘ã€äººç‰©å…³ç³»å’Œæ ¸å¿ƒäº‹ä»¶ã€‚ä½ çš„å·¥ä½œæ˜¯'æ¶¦è‰²'è€Œé'æ”¹ç¼–'ã€‚\n"
            "2. ä¸¥ç¦å‡ºç°ç°ä»£è¯æ±‡ï¼ˆå¦‚ï¼šæƒ…ç»ªã€å¿ƒç†ã€ç«‹åˆ»ã€æ­¤æ—¶ï¼‰ã€‚\n"
            "ã€é£æ ¼è¦æ±‚ã€‘ï¼š\n"
            "1. è¿˜åŸ'è„‚ç²‰æ°”'ä¸'ä¹¦å·æ°”'å¹¶å­˜çš„ç™½è¯æ–‡é£ã€‚\n"
            "2. å¯¹è¯è¦è¡¥å…¨ç¥æ€ï¼ˆå†·ç¬‘ã€ä½å¤´ã€æ‹­æ³ªï¼‰ã€‚\n"
            "3. ç« èŠ‚è¿‡æ¸¡ä½¿ç”¨'ä¸”è¯´'ã€'ä¸é¢˜'ã€'çœ‹å®˜å¬è¯´'ç­‰ä¼ ç»Ÿè¯æœ¯ã€‚"
        )
        
        self.cache = self._load_cache()

    def _load_cache(self) -> Dict[str, str]:
        if os.path.exists(CACHE_FILE):
            try:
                with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}

    def _save_cache(self):
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(self.cache, f, ensure_ascii=False, indent=2)

    def _get_hash(self, text: str) -> str:
        return hashlib.md5(text.encode('utf-8')).hexdigest()

    def smart_split(self, text: str, limit: int) -> List[str]:
        raw_paragraphs = text.split('\n')
        segments = []
        current_chunk = []
        current_len = 0

        for para in raw_paragraphs:
            para = para.strip()
            if not para: continue
            
            # ä¿æŠ¤å¯¹è¯å’Œè¯—è¯ä¸è¢«åˆ‡æ–­
            is_dialogue_start = para.endswith("é“ï¼š") or para.endswith("è¯´é“") or para.endswith("ç¬‘é“")
            
            current_chunk.append(para)
            current_len += len(para)

            if current_len >= limit and not is_dialogue_start:
                segments.append("\n".join(current_chunk))
                current_chunk = []
                current_len = 0
        
        if current_chunk:
            segments.append("\n".join(current_chunk))
        return segments

    async def refine_segment_async(self, task: SegmentTask, semaphore: asyncio.Semaphore) -> Optional[str]:
        input_hash = self._get_hash(task.text)
        if input_hash in self.cache:
            return self.cache[input_hash]

        user_content = f"""
{GOLDEN_SAMPLES}

==================================================
ã€å½“å‰è¿›åº¦ã€‘ï¼šæ­£åœ¨é‡å†™ã€Š{task.chapter_title}ã€‹çš„ç¬¬ {task.index}/{task.total} éƒ¨åˆ†ã€‚

ã€åŸå§‹åº•ç¨¿ã€‘ï¼š
{task.text}

ã€é‡å†™æŒ‡ä»¤ã€‘ï¼š
è¯·ä»¥æ­¤åº•ç¨¿ä¸ºéª¨æ¶ï¼Œé‡å¡‘è¡€è‚‰ã€‚ä¿ç•™æ‰€æœ‰æƒ…èŠ‚ï¼Œä»…ä¿®æ”¹è¯­è¨€é£æ ¼ã€‚
è¯·ç›´æ¥è¾“å‡ºé‡å†™åçš„æ­£æ–‡ï¼Œæ— éœ€ä»»ä½•è§£é‡Šæˆ–å‰ç¼€ã€‚
"""
        retries = 3
        async with semaphore:
            for attempt in range(retries):
                try:
                    # âœ… DeepSeek (OpenAI) è°ƒç”¨æ–¹å¼
                    response = await self.client.chat.completions.create(
                        model=self.model_name,
                        messages=[
                            {"role": "system", "content": self.system_prompt},
                            {"role": "user", "content": user_content}
                        ],
                        temperature=1.0, 
                        max_tokens=8192,
                        stream=False
                    )
                    
                    result_text = response.choices[0].message.content
                    if result_text:
                        self.cache[input_hash] = result_text
                        return result_text
                        
                except Exception as e:
                    wait_time = 2 ** attempt
                    logger.warning(f"[{task.chapter_title}-{task.index}] é‡è¯•({attempt+1}/{retries}): {e}")
                    await asyncio.sleep(wait_time)
            
            logger.error(f"[{task.chapter_title}-{task.index}] âŒ å½»åº•å¤±è´¥")
            return None

    async def process_chapter(self, file_path: str):
        if not os.path.exists(file_path): return

        filename = os.path.basename(file_path)
        title = filename.split('.')[0]
        output_file = os.path.join(OUTPUT_DIR, f"refined_{filename}")
        
        if os.path.exists(output_file):
            logger.info(f"â© è·³è¿‡å·²å­˜åœ¨: {filename}")
            return

        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        raw_segments = self.smart_split(content, CHUNK_SIZE)
        tasks = [SegmentTask(title, i+1, len(raw_segments), seg) for i, seg in enumerate(raw_segments)]

        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†: {title} (å…± {len(tasks)} ç‰‡æ®µ)")
        semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)
        
        results = await tqdm_asyncio.gather(
            *[self.refine_segment_async(t, semaphore) for t in tasks],
            desc=f"Refining {title}"
        )

        if None in results:
            logger.error(f"âŒ {title} å­˜åœ¨å¤±è´¥ç‰‡æ®µï¼Œè·³è¿‡ä¿å­˜ã€‚")
            return

        full_text = f"{title}\n\n" + "\n".join(results)
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(full_text)
        
        self._save_cache()
        logger.info(f"ğŸ’¾ {title} ä¿å­˜æˆåŠŸã€‚")

    async def run_batch(self, file_list: List[str]):
        if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)
        for file_path in file_list:
            await self.process_chapter(file_path)

# ================= æ ¸å¿ƒè¾…åŠ©å‡½æ•°ï¼šæ‹†åˆ†å¤§æ–‡ä»¶ï¼ˆV3.1 ç»ˆæä¿®æ­£ç‰ˆï¼‰ =================
def split_big_file_to_temp(raw_file: str, output_dir: str) -> List[str]:
    """
    ã€V3.1 ç»ˆæä¿®æ­£ç‰ˆã€‘
    1. ä½¿ç”¨æ­£åˆ™è¡Œé¦–åŒ¹é…ï¼Œè§£å†³â€œæ‰¹è¯­è¯¯åˆ‡â€é—®é¢˜ã€‚
    2. åŠ å…¥ã€æ™ºèƒ½å»é‡ã€‘ï¼Œè§£å†³â€œé‡å¤æ ‡é¢˜å¯¼è‡´æ–‡ä»¶åé”™ä½â€é—®é¢˜ã€‚
    """
    if not os.path.exists(raw_file):
        logger.error(f"âŒ æ‰¾ä¸åˆ°æºæ–‡ä»¶: {raw_file}")
        return []

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    with open(raw_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # æ­£åˆ™ï¼šåŒ¹é…è¡Œé¦–çš„â€œç¬¬xxå›â€ï¼Œä¸”å¿½ç•¥æ­£æ–‡ä¸­çš„å¼•ç”¨
    pattern = re.compile(r"(^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+å›\s+.+?)(?=^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+å›|\Z)", re.DOTALL | re.MULTILINE)
    
    matches = list(pattern.finditer(content))
    generated_files = []
    
    # ã€å»é‡å…³é”®ã€‘
    seen_chapters = set()
    real_index = 0
    
    if not matches:
        logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°æ ‡å‡†å›ç›®ï¼Œå…œåº•å¤„ç†ã€‚")
        out_path = os.path.join(output_dir, "chapter_full.txt")
        with open(out_path, "w", encoding='utf-8') as f: f.write(content)
        generated_files.append(out_path)
    else:
        logger.info(f"ğŸ“– æ‰«æåˆ° {len(matches)} ä¸ªç‰‡æ®µï¼Œæ­£åœ¨æ‰§è¡Œæ¸…æ´—å»é‡...")
        for m in matches:
            chapter_content = m.group(1)
            first_line = chapter_content.strip().split('\n')[0]
            
            # æå–å›ç›®å·ï¼ˆå¦‚â€œç¬¬ä¹åå…­å›â€ï¼‰ç”¨äºæŸ¥é‡
            chapter_num_match = re.search(r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+å›)", first_line)
            if not chapter_num_match: continue
            chapter_num = chapter_num_match.group(1)
            
            # 1. æŸ¥é‡
            if chapter_num in seen_chapters:
                logger.warning(f"ğŸ—‘ï¸ ä¸¢å¼ƒé‡å¤/è„šæ³¨ç‰‡æ®µ: {first_line[:20]}...")
                continue
            
            # 2. æŸ¥ç©ºï¼ˆè¿‡æ»¤åªæœ‰æ ‡é¢˜æ²¡æœ‰æ­£æ–‡çš„åºŸæ–™ï¼‰
            if len(chapter_content) < 50:
                logger.warning(f"ğŸ—‘ï¸ ä¸¢å¼ƒè¿‡çŸ­ç‰‡æ®µ: {first_line[:20]}...")
                continue
                
            seen_chapters.add(chapter_num)
            
            # 3. æ–‡ä»¶åç”Ÿæˆ
            safe_name = re.sub(r'[\\/*?:"<>|]', "", first_line[:30]).strip()
            safe_name = safe_name.replace(" ", "_").replace("[^1]", "")
            
            fname = f"{real_index + 81:03d}_{safe_name}.txt"
            out_path = os.path.join(output_dir, fname)
            
            with open(out_path, "w", encoding='utf-8') as f:
                f.write(chapter_content)
            generated_files.append(out_path)
            real_index += 1
            
    return generated_files

# ================= ä¸»æµç¨‹ =================
async def main():
    if API_KEY.startswith("xxxxx"):
        print("âŒ è¯·å…ˆå¡«å†™ DeepSeek API Key")
        return

    # 1. å‡†å¤‡å¼•æ“
    engine = RedChamberEngine(API_KEY, BASE_URL, MODEL_NAME)

    # 2. æ‹†åˆ†åŸå§‹æ–‡ä»¶ï¼ˆä½¿ç”¨å¸¦å»é‡çš„ V3.1 é€»è¾‘ï¼‰
    chapter_files = split_big_file_to_temp(FILE_28_RAW, TEMP_DIR)
    
    if not chapter_files:
        print("âŒ æ²¡æœ‰å¯å¤„ç†çš„æ–‡ä»¶ã€‚")
        return
        
    print(f"âœ… æˆåŠŸæ‹†åˆ†å‡º {len(chapter_files)} ä¸ªæœ‰æ•ˆç« èŠ‚ï¼Œå‡†å¤‡å¼€å§‹ç”Ÿæˆ...")

    # 3. æ‰¹é‡æ‰§è¡Œ
    await engine.run_batch(chapter_files)

# --- æ ‡å‡† Python è„šæœ¬å…¥å£ ---
if __name__ == "__main__":
    # ä½¿ç”¨ asyncio.run() æ¥è¿è¡Œé¡¶å±‚çš„å¼‚æ­¥å‡½æ•°
    # è¿™æ ·å°±å¯ä»¥æ­£ç¡®åœ°å¯åŠ¨äº‹ä»¶å¾ªç¯å¹¶æ‰§è¡Œ main()
    asyncio.run(main())